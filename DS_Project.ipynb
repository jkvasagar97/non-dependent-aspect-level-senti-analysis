{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "data = pd.read_csv(\"/Users/jeevakumar/Desktop/DS_Lab/data/project_data/UK_Farmer_Tweets.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data.keys()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "senti_data = data[['Message', 'Sentiment']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "senti_data.describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "senti_data.describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "senti_data.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "senti_data[senti_data.duplicated([\"Message\"],keep=False)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#senti_data['Sentiment'] = senti_data['Sentiment'].astype('category')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import nltk\n",
    "import string\n",
    "from nltk.tokenize import word_tokenize\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "\n",
    "def preprocess_text(text):\n",
    "    text = text.lower()\n",
    "    text = text.translate(str.maketrans(\"\", \"\", string.punctuation))\n",
    "    text = ''.join((z for z in text if not z.isdigit()))\n",
    "    # Tokenize text\n",
    "    tokens = nltk.word_tokenize(text)\n",
    "    # Remove stop words\n",
    "    stop_words = set(stopwords.words(\"english\"))\n",
    "    filtered_tokens = [token for token in tokens if token.lower() not in stop_words]\n",
    "    # Lemmatize tokens\n",
    "    lemmatizer = WordNetLemmatizer()\n",
    "    lemmatized_tokens = [lemmatizer.lemmatize(token) for token in filtered_tokens]\n",
    "    return lemmatized_tokens\n",
    "\n",
    "senti_data['tokenized_text'] = senti_data['Message'].apply(preprocess_text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# from nltk.tokenize import TweetTokenizer\n",
    "# tt = TweetTokenizer()\n",
    "# senti_data.Message = senti_data.Message.apply(tt.tokenize)\n",
    "senti_data['tokenized_text'] "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "senti_data.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Baseline model\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from sklearn.naive_bayes import MultinomialNB\n",
    "from sklearn.metrics import accuracy_score\n",
    "\n",
    "# Preprocess the data\n",
    "senti_data['Sentiment'] = senti_data['Sentiment'].map({'POSITIVE': 2, 'NEUTRAL': 1, 'NEGATIVE': 0})\n",
    "\n",
    "senti_data.dropna(inplace=True)\n",
    "# Split the data into training and testing sets\n",
    "X_train, X_test, y_train, y_test = train_test_split(senti_data['Message'], senti_data['Sentiment'], test_size=0.2)\n",
    "\n",
    "# Extract features from the tweets using bag-of-words representation\n",
    "vectorizer = CountVectorizer()\n",
    "X_train = vectorizer.fit_transform(X_train)\n",
    "X_test = vectorizer.transform(X_test)\n",
    "\n",
    "# Train the model using Naive Bayes classifier\n",
    "clf = MultinomialNB()\n",
    "clf.fit(X_train, y_train)\n",
    "\n",
    "# Test the model on the testing set and evaluate its performance\n",
    "y_pred = clf.predict(X_test)\n",
    "print('Accuracy:', accuracy_score(y_test, y_pred))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import spacy\n",
    "nlp = spacy.load('en_core_web_sm')\n",
    "\n",
    "def find_aspects_tweet(doc):\n",
    "    noun_adj_pairs = {}\n",
    "    doc = nlp(doc)\n",
    "    for chunk in doc.noun_chunks:\n",
    "        adj = set()\n",
    "        noun = \"\"\n",
    "        for tok in chunk:\n",
    "            if tok.pos_ == \"NOUN\":\n",
    "                noun = tok.text\n",
    "            if tok.pos_ == \"ADJ\":\n",
    "                adj.add(tok.text.lower())\n",
    "        if noun:\n",
    "            noun_adj_pairs.update({noun.lower():adj})\n",
    "    return noun_adj_pairs\n",
    "\n",
    "senti_data['aspects'] = senti_data['Message'].apply(find_aspects_tweet)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "aspects = {}\n",
    "# finding combining all aspects into a single dictionary\n",
    "for aspect in senti_data['aspects']:\n",
    "    for key, value in aspect.items():\n",
    "        if key not in aspects and len(value) > 0:\n",
    "            aspects[key] = set()\n",
    "        elif len(value) > 0:\n",
    "            aspects[key].update(value)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# finding common words associated with different aspects\n",
    "dep_aspects = {}\n",
    "count = 0 \n",
    "for key1, value1 in aspects.items():\n",
    "    for key2, value2 in aspects.items():\n",
    "        if key1 == key2: # continue if they are the same set\n",
    "            continue\n",
    "        keys = sorted([key1, key2])\n",
    "        combined_key = \"{}__{}\".format(keys[0], keys[1])\n",
    "        if combined_key in dep_aspects: # If the pair of aspects are already considered, ignore them\n",
    "            continue\n",
    "        inter = value1.intersection(value2)\n",
    "        if len(inter) > 0: # the sets are indepent with eachother\n",
    "            dep_aspects[combined_key] = inter\n",
    "\n",
    "# remove common words from aspect sets making them disjoint\n",
    "for keys, values in dep_aspects.items():\n",
    "    aspect1, aspect2 = keys.split(\"__\")\n",
    "    aspects[aspect1] = aspects[aspect1] - values\n",
    "    aspects[aspect2] = aspects[aspect2] - values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.10"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
